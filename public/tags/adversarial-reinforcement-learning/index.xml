<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adversarial Reinforcement Learning | Prakash Naikade</title>
    <link>http://localhost:1313/tags/adversarial-reinforcement-learning/</link>
      <atom:link href="http://localhost:1313/tags/adversarial-reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Adversarial Reinforcement Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 24 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu5efb93123565b269d61ea7706dd6c107_780703_512x512_fill_lanczos_center_3.png</url>
      <title>Adversarial Reinforcement Learning</title>
      <link>http://localhost:1313/tags/adversarial-reinforcement-learning/</link>
    </image>
    
    <item>
      <title>Adversarial Reinforcement Learning</title>
      <link>http://localhost:1313/teaching/adversarial-rl/</link>
      <pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/adversarial-rl/</guid>
      <description>&lt;p&gt;Research papers covered in this post will showcase the landscape of attacks on RL agents and the optimal attack strategies, which is crucial for understanding security threats against the deployed systems. In particular, the research papers will cover optimal attack strategies for test-time, backdoor, and training-time (environment poisoning) attacks on RL agents. These research papers provides better perspective of important problems for developing robust and secure algorithms in sequential decision-making settings. This repo is result of &lt;a href=&#34;https://machineteaching.mpi-sws.org/course-adversarialrl-w21.html&#34;&gt;Course - Adversarial Reinforcement Learning&lt;/a&gt;, I followed at Saarland University.&lt;/p&gt;
&lt;h2 id=&#34;list-of-research-papers-covered&#34;&gt;List of research papers covered&lt;/h2&gt;
&lt;h3 id=&#34;test-time-attacks&#34;&gt;Test-time attacks&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Tactics of Adversarial Attack on Deep Reinforcement Learning Agents
by Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, at IJCAI 2017.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning
by J. Sun, T. Zhang, X. Xie, L. Ma, Y. Zheng, K. Chen, and Y. Liu, at AAAI 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations
by H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C. Hsieh, at NeurIPS 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;backdoor-attacks&#34;&gt;Backdoor attacks&lt;/h3&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning
by P. Kiourti, K. Wardega, S. Jha, and W. Li, at DAC 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal Watermarks for Deep Reinforcement Learning Modelss
by K. Chen, S. Guo, T. Zhang, S. Li, and Y. Liu, at AAMAS 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-time-attacks&#34;&gt;Training-time attacks&lt;/h3&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;
&lt;p&gt;Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning
by A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla, at ICML 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics
by Y. Sun, D. Huo, and F. Huang, at ICLR 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Defense Against Reward Poisoning Attacks in Reinforcement Learning
by K. Banihashem, A. Singla, and G. Radanovic, at arXiv preprint 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summaryoverview&#34;&gt;Summary/Overview&lt;/h2&gt;
&lt;p&gt;Find the summary of above papers &lt;a href=&#34;https://github.com/prakashknaikade/Adversarial-Reinforcement-Learning-Key-Research-Papers&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
