<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Prakash Naikade</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Prakash Naikade</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu5efb93123565b269d61ea7706dd6c107_780703_512x512_fill_lanczos_center_3.png</url>
      <title>Prakash Naikade</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>3D AI</title>
      <link>http://localhost:1313/project/3d-ai/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/3d-ai/</guid>
      <description>&lt;p&gt;In this repo I implement basic 3D Computer Vision (3D CV), Graphics and 3D AI techniques for learning purpose.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ðŸ§  Open to work</title>
      <link>http://localhost:1313/post/job/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/job/</guid>
      <description>&lt;p&gt;I am a passionate AI researcher with extensive experience and interests in the expansive fields of Neural Rendering, Radiance Field Methods, Novel View Synthesis, Motion Capture, 3D-Reconstruction, and Digital Twins. My expertise also covers cutting-edge areas such as AR/VR, Generative AI, Large Language Models (LLMs), Human-Computer Interaction (HCI), and broadly within Computer Vision, Computer Graphics, Deep Learning, Machine Learning, and Data Science. I am driven to apply my skills to solve real-world problems with impactful, AI-aided solutions.&lt;/p&gt;
&lt;h2 id=&#34;research-interests&#34;&gt;Research Interests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ‘‰ Neural Rendering &amp;amp; Radiance Field Methods: Developing advanced techniques for realistic rendering and visualization of 3D scenes from sparse data.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Novel View Synthesis: Creating new perspectives of scenes using machine learning models.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Motion Capture &amp;amp; 3D Reconstruction: Innovating in the capture and reconstruction of dynamic 3D environments and human motions.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Digital Twins &amp;amp; AR/VR: Building digital replicas and immersive experiences for enhanced interaction and simulation.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Generative AI &amp;amp; LLMs: Leveraging generative models and large language models for creative and predictive tasks.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Human-Computer Interaction: Enhancing user interfaces and interactions through intelligent design and feedback.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Computer Vision &amp;amp; Computer Graphics: Pioneering in the analysis, interpretation, and generation of visual data.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Deep/Machine Learning &amp;amp; Data Science: Applying sophisticated algorithms and data-driven methods to extract insights and drive innovation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-i-offer&#34;&gt;What I Offer&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ‘‰ Innovative Solutions: Proven ability to develop and deploy AI-driven solutions that address complex problems and improve efficiency.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Collaborative Mindset: Experience working in interdisciplinary teams to push the boundaries of technology and research.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Research Excellence: Strong track record of publications, presentations, and contributions to the scientific community.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Technical Proficiency: Proficient in programming languages such as Python, C++, and tools like TensorFlow, PyTorch, and more.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Problem-Solving Skills: Adept at identifying challenges and devising creative, effective solutions using AI and machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;looking-for&#34;&gt;Looking For&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ðŸ‘‰ Research Opportunities: Positions in academia, industry, or research institutions where I can contribute to and lead groundbreaking AI projects.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Collaborative Projects: Partnerships with like-minded researchers and organizations aimed at pushing the frontiers of AI and computer vision.&lt;/li&gt;
&lt;li&gt;ðŸ‘‰ Innovative Environments: Workplaces that value creativity, innovation, and the application of AI to solve real-world problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;contact&#34;&gt;Contact&lt;/h2&gt;
&lt;p&gt;If you are looking for a dedicated researcher with a deep understanding of advanced AI and computer vision techniques, I would love to connect. Please feel free to reach out to discuss potential opportunities and collaborations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Novel View Synthesis of Structural Color Objects Created by Laser Markings</title>
      <link>http://localhost:1313/publications/nvs_structural_color_object/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/nvs_structural_color_object/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p style=&#34;font-size: 1.1rem;text-align: justify;&#34;&gt;
Transforming physical object into its high quality 3D digital twin using novel view synthesis is crucial for researchers in the domain of automatic laser marking of any color image on different metal substrates. Current Radiance Field methods have significantly advanced novel view synthesis of scenes captured with multiple photos or videos. But, they struggle to represent the scene with shiny objects. Moreover, multi view reconstruction of reflective objects with structural colors is extremely challenging because specular reflections are view-dependent and thus violate the multiviewconsistency, which is the cornerstone for most multiview reconstruction methods. However, there is a general lack of synthetic datasets for objects with structural colors and a literature review on state-of-the-art (SOTA) novel view synthesis methods for this kind of materials. Addressing these issues, we introduce a novel synthetic dataset that is used to conduct quantitative and qualitative analysis on a SOTA novel view synthesis methods. We demonstrate different techniques to improve the scene representation of laser printed planar structural color objects, focusing on the 3D Gaussian Splatting (3D-GS) method, which performs exceptionally well on our synthetic dataset. Our techniques, such as using geometric prior of planar structural color objects while initializing scene with sparse structure-from-motion (SfM) point cloud and the Anisotropy Regularizer, significantly improves the visual quality of view synthesis. We design different capture setups to acquire images of objects and evaluate the visual quality of the scene with different capture setups. Additionally, we present comprehensive experimentation to demonstrate methods to simulate structural color objects using just captured images of laser-printed primaries. This comprehensive research aims to contribute to the advancement of novel view synthesis methods for scenes involving reflective objects with structural colors.
&lt;/p&gt;
&lt;h3 id=&#34;tldr&#34;&gt;TLDR&lt;/h3&gt;
&lt;p&gt;This work is the result of my master thesis on &amp;ldquo;Novel View Synthesis of Structural Color Objects Created by Laser Markings&amp;rdquo;, in collaboration with &lt;a href=&#34;https://aidam.mpi-inf.mpg.de/?view=home&#34;&gt;AIDAM&lt;/a&gt;, &lt;a href=&#34;https://www.oraclase.com/&#34;&gt;Oraclase&lt;/a&gt;, &lt;a href=&#34;https://www.mpi-inf.mpg.de/home/&#34;&gt;MPI-Inf&lt;/a&gt;, and &lt;a href=&#34;https://saarland-informatics-campus.de/en/&#34;&gt;SIC&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p style=&#34;font-size: 1.1rem;&#34;&gt;
This work marks the beginning of Novel View Synthesis (NVS) of structural color objects and opens up potential avenues for various future research directions and endeavors. 
&lt;/p&gt;
&lt;p style=&#34;font-size: 1.1rem;&#34;&gt;
&lt;b&gt;Beginners&lt;/b&gt;: This thesis will help beginners grasp the brief history of radiance field methods and essential preliminaries, such as the mathematics, tools, and concepts necessary to understand NVS methods. Furthermore, it will serve as a foundational guide for novice researchers interested in exploring the novel view synthesis of specular and shiny objects with high view-dependent colors, commonly referred to as Structural Colors or Pearlescent Colors.
&lt;/p&gt;
&lt;p style=&#34;font-size: 1.1rem;&#34;&gt;
&lt;b&gt;Advanced Researchers&lt;/b&gt;: This thesis will also be a valuable reference for advanced researchers who may wish to revisit the fundamental concepts and access the &lt;em&gt;StructColorToaster scene&lt;/em&gt; from new &lt;em&gt;Structural Color Blender Dataset&lt;/em&gt;. Additionally, the thesis discusses approaches to improve results and outlines potential future research directions.
&lt;/p&gt;
&lt;p style=&#34;font-size: 1.1rem;&#34;&gt;
&lt;b&gt;Technical Artists&lt;/b&gt;: This thesis provides an overview of how the new Gaussian Splatting technology can be applied to various use cases and demonstrates the life cycle of a specific use case in interactive product visualization, from capturing the product to visualizing it in a web viewer. 
&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;h3 id=&#34;datasets&#34;&gt;Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Structural Color Blender Dataset (Synthetic Scenes)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;StructColorToaster&lt;/em&gt; Scene: Access Blender source and dataset from ðŸ‘‰&lt;a href=&#34;https://www.dropbox.com/scl/fo/9btslfn5y71lb6ct4jy62/AJ-6C-8QmycAND85arMPBDQ?rlkey=tqq444p2k608el58aoq5tzbvf&amp;amp;e=1&amp;amp;st=qcq9xn97&amp;amp;dl=0&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Real Scenes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;StructColorPainting&lt;/em&gt; Scene&lt;/li&gt;
&lt;li&gt;&lt;em&gt;StructColorPaintingOld&lt;/em&gt; Scene&lt;/li&gt;
&lt;li&gt;&lt;em&gt;StructColorPrimaries&lt;/em&gt; Scene&lt;/li&gt;
&lt;li&gt;&lt;em&gt;StructColorTaylorSwift&lt;/em&gt; Scene&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;h4 id=&#34;structural-color-blender-dataset-synthetic-scenes&#34;&gt;Structural Color Blender Dataset (Synthetic Scenes)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of selected SOTA methods on &lt;em&gt;StructColorToaster&lt;/em&gt; scene:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;width: 100%; max-width: 600px; margin: auto;&#34;&gt;
  &lt;table style=&#34;font-size: 0.9rem; padding: 2px; margin: auto; border-collapse: collapse;&#34;&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;Method&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;Iterations&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;PSNR &amp;#8593;&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;SSIM &amp;#8593;&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;LPIPS &amp;#8595;&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;Train&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;FPS&lt;/th&gt;
        &lt;th style=&#34;padding: 2px;&#34;&gt;Memory&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;NeRF&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;50K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;23.359&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.8427&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.1895&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;1 day&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.001&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;22MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;InstantNGP&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;50K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;19.659&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.8109&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.2068&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;23 min&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;6&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;159.2MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;Mip-NeRF&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;250K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;22.818&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.877&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.1263&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;1 day&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.03&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;15.9MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;NeRFacto&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;50K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;19.305&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.8056&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.2123&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;35 min&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.6&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;168MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;Ref-NeRF&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;250K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;27.5194&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.9142&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.1286&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;2 days&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.3&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;8.2MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px; font-weight: bold;  background-color: yellow; color: #000;&#34;&gt;3D-GS&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px; font-weight: bold;  background-color: yellow; color: #000;&#34;&gt;24.0435&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.9065&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.1061&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;20 min&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;140&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;77MB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;3D-GS&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;60K&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;24.3117&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.9070&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;0.1052&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;30 min&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;140&lt;/td&gt;
        &lt;td style=&#34;padding: 2px;&#34;&gt;77MB&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
  &lt;figcaption style=&#34;text-align: left; margin-top: 10px;&#34;&gt;
    Table 1: Quantitative evaluation of selected methods&#39; results computed over our &lt;em&gt;StructColorToaster&lt;/em&gt; Scene. &lt;br&gt; (The values for the Train time and FPS are approximate.)
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Renders of optimized &lt;em&gt;StructColorToaster&lt;/em&gt; Scene using 3D-GS:
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publications/nvs_structural_color_object/3DGS_Toaster.jpeg&#34;
    alt=&#34;Figure 1: The renderings show that 3D-GS able to capture the crisp shininess appearance of the surface. However, it does improve in learning specular reflections with increasing iterations, it is still not up to the mark and suffures from  holes and aliasing effect aka popping artifacts.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Figure 1: The renderings show that 3D-GS able to capture the crisp shininess appearance of the surface. However, it does improve in learning specular reflections with increasing iterations, it is still not up to the mark and suffures from  holes and aliasing effect aka popping artifacts.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;real-scenes&#34;&gt;Real Scenes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Evalution of different choices on &lt;em&gt;StructColorPainting&lt;/em&gt; and &lt;em&gt;StructColorPaintingOld&lt;/em&gt; Scenes&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;width: 100%; max-width: 600px; margin: auto;&#34;&gt;
&lt;table style=&#34;font-size: 0.9rem; padding: 2px; margin: auto; border-collapse: collapse;&#34;&gt;
    &lt;thead&gt;
        &lt;tr&gt;
            &lt;th style=&#34;padding: 2px;&#34;&gt;Method&lt;/th&gt;
            &lt;th style=&#34;padding: 2px;&#34;&gt;7K&lt;/th&gt;
            &lt;th style=&#34;padding: 2px;&#34;&gt;30K&lt;/th&gt;
        &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Vanilla 3D-GS&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.11&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;28.92&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Cleaned-SfM&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.6715&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;  font-weight: bold;  background-color: yellow; color: #000;&#34;&gt;29.1789&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;With-Masks&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;9.352&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;9.4603&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;With-Cropped-Images/RGBA Images&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.1759&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;30.0007&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt; With-Cropped-Images/RGBA Images: &lt;br&gt; (No-Densification + No-Position Optimization)&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;24.4612&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;25.089&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Densify Until Iteration = 30000&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;26.8325&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.4149&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Densification Interval = 30&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;25.2634&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;28.6454&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Densification Interval = 50&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;26.0189&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;28.2718&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Reset Opacity = Every 1000 Iteration&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.7216&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.2581&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Reset Opacity = Every 2000 Iteration&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.6354&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.1786&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Reset Opacity = Every 5000 Iteration&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.82&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.2726&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;SH-Degree 0&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;26.4699&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.9771&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;White-Background&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.6579&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.2025&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Random-Background&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.7607&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;29.3303&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;No-Densification&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;17.7919&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;18.3929&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Reduced Number of Images = 297&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.3657&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;28.7223&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;Anisotropy-Regularizer&lt;/td&gt;
            &lt;td style=&#34;padding: 2px;&#34;&gt;27.5747&lt;/td&gt;
            &lt;td style=&#34;padding: 2px; font-weight: bold;  background-color: yellow; color: #000;&#34;&gt;29.4191&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;figcaption style=&#34;text-align: left; margin-top: 10px;&#34;&gt;Table 2: PSNR Score for ablation runs. Quantitative evaluation of results computed over our &lt;em&gt;StructColorPainting&lt;/em&gt; scene using different 3D-GS settings. (The values for FPS are approximate.)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of selected techniquesâ€™ results computed over our &lt;em&gt;StructColorPainting&lt;/em&gt; scene:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;width: 100%; max-width: 600px; margin: auto;&#34;&gt;
    &lt;table style=&#34;font-size: 0.9rem; padding: 2px; margin: auto; border-collapse: collapse;&#34;&gt;
        &lt;thead&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Method&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Iterations&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;PSNR &amp;#8593;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;SSIM &amp;#8593;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;LPIPS &amp;#8595;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Train&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;FPS&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Memory&lt;/th&gt;
            &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;Cleaned-SfM&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;27.6452&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9221&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1795&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;5.122min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;79MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;29.1789&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9342&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1602&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;27.69min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;Anisotropy-Regularizer&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;27.5476&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9223&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1780&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;5.134min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;78.6MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;29.9631&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9346&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; &#34;&gt;0.1572&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;26.35 min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;152.4MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;Anisotropy-Regularizer:&lt;br&gt;297-Images&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;26.9920&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9238&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1833&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;5.812 min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;86.1MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;29.3213&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9339&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1648&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;28.57min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;176.2MB&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;figcaption style=&#34;text-align: left; margin-top: 10px;&#34;&gt;
            Table 3: Quantitative evaluation of selected techniquesâ€™ results computed over our &lt;em&gt;StructColorPainting&lt;/em&gt; scene. (The values for FPS are approximate).
        &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Evaluation of renderings using our techniquesâ€™ and gsplat with cleaned SfM and additional features computed over &lt;em&gt;StructColorTaylorSwift&lt;/em&gt; scene:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;width: 100%; max-width: 600px; margin: auto;&#34;&gt;
    &lt;table style=&#34;font-size: 0.9rem; padding: 2px; margin: auto; border-collapse: collapse;&#34;&gt;
        &lt;thead&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Method&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Iterations&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;PSNR &amp;#8593;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;SSIM &amp;#8593;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;LPIPS &amp;#8595;&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Train&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;FPS&lt;/th&gt;
                &lt;th style=&#34;padding: 2px;&#34;&gt;Memory&lt;/th&gt;
            &lt;/tr&gt;
        &lt;/thead&gt;
        &lt;tbody&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;Our Techniques&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30.2567&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9338&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1921&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;4.123min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;55MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;33.6547&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9458&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1749&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;18.5min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;94MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;gsplat&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;31.8051&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9391&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1880&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;2.86min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;64MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;34.6451&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9511&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1658&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;16.61min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;119MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;gsplat + Mip-Splatting&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;7K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;31.3528&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9362&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1922&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;2.54min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;150&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;66MB&lt;/td&gt;
            &lt;/tr&gt;
            &lt;tr style=&#34;border-bottom: 3px solid grey;&#34;&gt;
                &lt;td&gt;&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;30K&lt;/td&gt;
                &lt;td style=&#34;padding: 2px; font-weight: bold; background-color: yellow; color: #000;&#34;&gt;34.6654&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.9379&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;0.1827&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;17.1min&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;120&lt;/td&gt;
                &lt;td style=&#34;padding: 2px;&#34;&gt;124MB&lt;/td&gt;
            &lt;/tr&gt;
        &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;figcaption style=&#34;text-align: left; margin-top: 10px;&#34;&gt;
        Table 4: Quantitative evaluation of selected techniquesâ€™ results computed over our &lt;em&gt;StructColorPainting&lt;/em&gt; scene. (The values for FPS are approximate.)
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;videos&#34;&gt;Videos&lt;/h3&gt;
&lt;h4 id=&#34;real-scenes-1&#34;&gt;Real Scenes&lt;/h4&gt;
&lt;p&gt;RGB renders of interactive visualization in SIBR viewer of an optimized real world sences with our techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;StructColorPainting&lt;/em&gt; scene











  





&lt;video controls  &gt;
  &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/man_16_891_imgs.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;StructColorTaylorSwift&lt;/em&gt; scene











  





&lt;video controls  &gt;
  &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/taylor_simulation_2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h4 id=&#34;synthesized-scenes-using-just-primaries&#34;&gt;Synthesized Scenes Using Just Primaries&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Simulating Structural Color Object (Pseudo) before Laser Printing:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/publications/nvs_structural_color_object/pipeline_gsplat.jpg&#34;
    alt=&#34;Figure 2: Pipeline of Synthesizing Arbitrary Images of Structural Color Object for Arbitrary Viewing Directions.&#34;&gt;&lt;figcaption&gt;
      &lt;p&gt;Figure 2: Pipeline of Synthesizing Arbitrary Images of Structural Color Object for Arbitrary Viewing Directions.&lt;/p&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Comparison between real structural color object views and respective synthesized views using just primaries for respective view directions:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;StructColorTaylorSwift&lt;/em&gt; scene

  
  
  
  
  
  
  
  
  
  
    
  
  
  
  
  
  &lt;video controls  &gt;
    &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/taylor_swift_synth_views_gsplat_compare_%20720p.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RGB renders of interactive visualization in SIBR viewer of an optimized synthetic scene, generated using synthesized images created just with primaries:

  
  
  
  
  
  
  
  
  
  
    
  
  
  
  
  
  &lt;video controls  &gt;
    &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/taylor_synth_perspective_simulation.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;Synthesizing views using only the earlier tracked primaries for respective view directions, before laser printing the structural color object:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Synthesized views using just primaries for respective view directions:

  
  
  
  
  
  
  
  
  
  
    
  
  
  
  
  
  &lt;video controls  &gt;
    &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/cat_synth_views.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RGB renders of interactive visualization in SIBR viewer of an optimized synthetic scene, generated using synthesized images created just with primaries:

  
  
  
  
  
  
  
  
  
  
    
  
  
  
  
  
  &lt;video controls  &gt;
    &lt;source src=&#34;http://localhost:1313/publications/nvs_structural_color_object/cat_synth_perspective_simulation.mp4&#34; type=&#34;video/mp4&#34;&gt;
  &lt;/video&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3d-web-viewer&#34;&gt;3D Web Viewer&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://prakashknaikade.github.io/StructColorPaintingViewer/&#34;&gt;&lt;strong&gt;StructColorPaintingViewer&lt;/strong&gt;&lt;/a&gt; is a webviwer to visulaize the structural color objects.
The webviewer is implemented with help of &lt;a href=&#34;https://github.com/huggingface/gsplat.js&#34;&gt;gsplat.js&lt;/a&gt; but it only supports scenes optimized with spherical harmonics (SH) of degree 0. However, scenes optimized with SH0 don&amp;rsquo;t achieve the same quality as those optimized with SH3, which is evident from above videos of renderings of scenes optimized with SH3 in SIBR viewer.&lt;/p&gt;
&lt;iframe src=&#34;https://prakashknaikade.github.io/StructColorPaintingViewer/&#34;
        width=&#34;100%&#34; height=&#34;600px&#34;
        style=&#34;border: none;&#34;&gt;
&lt;/iframe&gt;
&lt;h3 id=&#34;bibtex&#34;&gt;BibTeX&lt;/h3&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p style=&#34;font-size: 1.1rem;&#34;&gt;
1. [NeRF] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020.&lt;br&gt;  
2. [InstantNGP] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans.Graph., July 2022.&lt;br&gt; 
3. [Mip-NeRF] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. ICCV, 2021.&lt;br&gt;
4. [NeRFacto] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. Nerfstudio: A modular framework for neural radiance field development. ACM SIGGRAPH 2023.&lt;br&gt;
5. [Ref-NeRF] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR, 2022.&lt;br&gt; 
6. [3D-GS] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, July 2023.
&lt;/p&gt; 
</description>
    </item>
    
    <item>
      <title>Diffusion Models</title>
      <link>http://localhost:1313/project/diffusion-models/</link>
      <pubDate>Tue, 25 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/diffusion-models/</guid>
      <description>&lt;p&gt;This repo is a basic implementation of Diffusion Model to understand how diffusion works. This is the outcome of project course &lt;a href=&#34;https://www.coursera.org/projects/how-diffusion-models-work-project&#34;&gt;How Diffusion Models&lt;/a&gt; Work that I completed.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Research-Papers-I-Read</title>
      <link>http://localhost:1313/teaching/papers-i-read/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/papers-i-read/</guid>
      <description>&lt;p&gt;&lt;strong&gt;1. Deep SDF: Learning Continuous Signed Distance Functions for Shape Rpresenation&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/DeepSDF-Paper.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. AI for Sports: Human Pose Estimation&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/Human-Pose-Estimation.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Long-term Human Motion Prediction with Scene Context&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/Human-Motion-Prediction.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. PoseFix: Correcting 3D Human Poses with Natural Language&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/PoseFix_Correcting-3D-Human-Poses-with-Natural-Language.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Corrosion Detection and Progression Using Computer Vision&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/Corrosion-Detection-Progression.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. Tactics of Adversarial Attack on Deep Reinforcement Learning Agents&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://github.com/prakashknaikade/Papers-I-Read/blob/main/Tactics-of-Adversarial-Attack-on-Deep-Reinforcement-Learning-Agents.pdf&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7. Novel View Synthesis using Radiance Field Methods: NeRF and Instant-NGP&lt;/strong&gt;
&lt;br&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1HAcz7obNXgOj83u2HZmXaANHG31Tw3cxV1-rgsaYjNU/edit?usp=sharing&#34;&gt;Read Summary&lt;/a&gt;&lt;/p&gt;
&lt;!-- [Hugo Blox Builder](https://hugoblox.com) is designed to give technical content creators a seamless experience. You can focus on the content and the Hugo Blox Builder which this template is built upon handles the rest.

**Embed videos, podcasts, code, LaTeX math, and even test students!**

On this page, you&#39;ll find some examples of the types of technical content that can be rendered with Hugo Blox.

## Video

Teach your course by sharing videos with your students. Choose from one of the following approaches:



    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/D2vj0WcvH5c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;


**Youtube**:

    {{&lt; youtube w7Ft2ymGmfc &gt;}}

**Bilibili**:

    {{&lt; bilibili id=&#34;BV1WV4y1r7DF&#34; &gt;}}

**Video file**

Videos may be added to a page by either placing them in your `assets/media/` media library or in your [page&#39;s folder](https://gohugo.io/content-management/page-bundles/), and then embedding them with the _video_ shortcode:

    {{&lt; video src=&#34;my_video.mp4&#34; controls=&#34;yes&#34; &gt;}}

## Podcast

You can add a podcast or music to a page by placing the MP3 file in the page&#39;s folder or the media library folder and then embedding the audio on your page with the _audio_ shortcode:

    {{&lt; audio src=&#34;ambient-piano.mp3&#34; &gt;}}

Try it out:









  








&lt;audio controls &gt;
  &lt;source src=&#34;ambient-piano.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;


## Test students

Provide a simple yet fun self-assessment by revealing the solutions to challenges with the `spoiler` shortcode:

```markdown
{{&lt; spoiler text=&#34;ðŸ‘‰ Click to view the solution&#34; &gt;}}
You found me!
{{&lt; /spoiler &gt;}}
```

renders as

&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ Click to view the solution&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    You found me ðŸŽ‰
  &lt;/div&gt;
&lt;/details&gt;

## Math

Hugo Blox Builder supports a Markdown extension for $\LaTeX$ math. You can enable this feature by toggling the `math` option in your `config/_default/params.yaml` file.

To render _inline_ or _block_ math, wrap your LaTeX math with `{{&lt; math &gt;}}$...${{&lt; /math &gt;}}` or `{{&lt; math &gt;}}$$...$${{&lt; /math &gt;}}`, respectively.






  
    
  

&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;We wrap the LaTeX math in the Hugo Blox &lt;em&gt;math&lt;/em&gt; shortcode to prevent Hugo rendering our math as Markdown.&lt;/span&gt;
&lt;/div&gt;

Example **math block**:

```latex
{{&lt; math &gt;}}
$$
\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}
$$
{{&lt; /math &gt;}}
```

renders as


$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$



Example **inline math** `{{&lt; math &gt;}}$\nabla F(\mathbf{x}_{n})${{&lt; /math &gt;}}` renders as $\nabla F(\mathbf{x}_{n})$
.

Example **multi-line math** using the math linebreak (`\\`):

```latex
{{&lt; math &gt;}}
$$f(k;p_{0}^{*}) = \begin{cases}p_{0}^{*} &amp; \text{if }k=1, \\
1-p_{0}^{*} &amp; \text{if }k=0.\end{cases}$$
{{&lt; /math &gt;}}
```

renders as



$$
f(k;p_{0}^{*}) = \begin{cases}p_{0}^{*} &amp; \text{if }k=1, \\
1-p_{0}^{*} &amp; \text{if }k=0.\end{cases}
$$




## Code

Hugo Blox Builder utilises Hugo&#39;s Markdown extension for highlighting code syntax. The code theme can be selected in the `config/_default/params.yaml` file.


    ```python
    import pandas as pd
    data = pd.read_csv(&#34;data.csv&#34;)
    data.head()
    ```

renders as

```python
import pandas as pd
data = pd.read_csv(&#34;data.csv&#34;)
data.head()
```

## Inline Images

```go
{{&lt; icon name=&#34;python&#34; &gt;}} Python
```

renders as


  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python

## Did you find this page helpful? Consider sharing it ðŸ™Œ --&gt;
</description>
    </item>
    
    <item>
      <title>Learn-LLMs</title>
      <link>http://localhost:1313/project/llms/</link>
      <pubDate>Wed, 15 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/llms/</guid>
      <description>&lt;p&gt;This Project is result of getting a hands-on experience of using different LLM models and tools. I am going to use a lot of tools and practice with small projects that will grow as I apply the new knowledge acquired. This repo is going to be a basic implementation of different LLMs to understand the finetuning, data preparation, evaluation &amp;amp; other techniques related to/of LLMs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ðŸ’¼ Experience</title>
      <link>http://localhost:1313/experience/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Action Recognition (HAR)</title>
      <link>http://localhost:1313/project/har/</link>
      <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/har/</guid>
      <description>&lt;p&gt;Investigating the performance of different deep learning models and their ensembles used for HAR in still images.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Mini-RayTracer</title>
      <link>http://localhost:1313/project/mini-ray-tracer/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/mini-ray-tracer/</guid>
      <description>&lt;p&gt;Developing simple ray tracing engine in C++.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>http://localhost:1313/projects/</link>
      <pubDate>Thu, 02 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adversarial Reinforcement Learning</title>
      <link>http://localhost:1313/teaching/adversarial-rl/</link>
      <pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/teaching/adversarial-rl/</guid>
      <description>&lt;p&gt;Research papers covered in this post will showcase the landscape of attacks on RL agents and the optimal attack strategies, which is crucial for understanding security threats against the deployed systems. In particular, the research papers will cover optimal attack strategies for test-time, backdoor, and training-time (environment poisoning) attacks on RL agents. These research papers provides better perspective of important problems for developing robust and secure algorithms in sequential decision-making settings. This repo is result of &lt;a href=&#34;https://machineteaching.mpi-sws.org/course-adversarialrl-w21.html&#34;&gt;Course - Adversarial Reinforcement Learning&lt;/a&gt;, I followed at Saarland University.&lt;/p&gt;
&lt;h2 id=&#34;list-of-research-papers-covered&#34;&gt;List of research papers covered&lt;/h2&gt;
&lt;h3 id=&#34;test-time-attacks&#34;&gt;Test-time attacks&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Tactics of Adversarial Attack on Deep Reinforcement Learning Agents
by Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, at IJCAI 2017.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stealthy and Efficient Adversarial Attacks against Deep Reinforcement Learning
by J. Sun, T. Zhang, X. Xie, L. Ma, Y. Zheng, K. Chen, and Y. Liu, at AAAI 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations
by H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, and C. Hsieh, at NeurIPS 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;backdoor-attacks&#34;&gt;Backdoor attacks&lt;/h3&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;
&lt;p&gt;TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning
by P. Kiourti, K. Wardega, S. Jha, and W. Li, at DAC 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Temporal Watermarks for Deep Reinforcement Learning Modelss
by K. Chen, S. Guo, T. Zhang, S. Li, and Y. Liu, at AAMAS 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;training-time-attacks&#34;&gt;Training-time attacks&lt;/h3&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;
&lt;p&gt;Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning
by A. Rakhsha, G. Radanovic, R. Devidze, X. Zhu, and A. Singla, at ICML 2020.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics
by Y. Sun, D. Huo, and F. Huang, at ICLR 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Defense Against Reward Poisoning Attacks in Reinforcement Learning
by K. Banihashem, A. Singla, and G. Radanovic, at arXiv preprint 2021.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summaryoverview&#34;&gt;Summary/Overview&lt;/h2&gt;
&lt;p&gt;Find the summary of above papers &lt;a href=&#34;https://github.com/prakashknaikade/Adversarial-Reinforcement-Learning-Key-Research-Papers&#34;&gt;Here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Object Detection</title>
      <link>http://localhost:1313/project/object-detection/</link>
      <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/object-detection/</guid>
      <description>&lt;p&gt;In this notebook we will go over on how to train a object detection model on custom dataset using TensorFlow Object Detection API 2. For this purpose I used Oxford Pets dataset. We will monitor model training using TensorBoard.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Easy Flappy Bird</title>
      <link>http://localhost:1313/project/easy-flappy-bird/</link>
      <pubDate>Fri, 25 Nov 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/easy-flappy-bird/</guid>
      <description>&lt;p&gt;This repo is simple implementation of Flappy Bird game using Unity and C# for learning purpose.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Covid19 Detection</title>
      <link>http://localhost:1313/project/covid19_detection/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/covid19_detection/</guid>
      <description>&lt;p&gt;This project is aimed at covid19 detection using pretrained ResNet50 model.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Synthetic Dataset Creation</title>
      <link>http://localhost:1313/project/synthetic-dataset/</link>
      <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/synthetic-dataset/</guid>
      <description>&lt;p&gt;This repo is simple demo of synthetic data creation using Blender and Python.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chatbot - Villa Nennig</title>
      <link>http://localhost:1313/project/chatbot/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/chatbot/</guid>
      <description>&lt;p&gt;This chatbot helps the user throughout their journey of visiting a museum of the Roman Villa Nennig! We developed this chatbot using Google Cloud, Dialogflow Essentials and Telegram.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Ludwig-Palette: an AR painting game</title>
      <link>http://localhost:1313/project/ludwig-palette/</link>
      <pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/ludwig-palette/</guid>
      <description>&lt;p&gt;Objective of game is to help visitors of the Ludwig church, a historical church in SaarbrÃ¼cken, explore the environment of the church in an entertaining way, we have created an AR-enabled painting app to be played inside the church. Players of this game need to walk to the different areas of the church while painting the surfaces. The app is developed using Unity and Vuforia engine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Image Segmentation</title>
      <link>http://localhost:1313/project/image-segmentation/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/image-segmentation/</guid>
      <description>&lt;p&gt;This computer vision project is the pytorch implementations of UNet architecture for image segmenation on psacalVOC dataset to understand how CNNs are utilized for segmentation task.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Secure Data Storage on Multi-Cloud Using DNA Based Cryptography</title>
      <link>http://localhost:1313/publications/dna_based_cryptography/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/dna_based_cryptography/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
